import os
import time
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFLoader, TextLoader, CSVLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from pinecone import Pinecone

# --- 1. CONFIGURATION ---
load_dotenv()

DATA_PATH = "data_clean"
INDEX_NAME = "expert-social"

def run_incremental_ingestion():
    # Connexion √† Pinecone (pour les suppressions cibl√©es)
    try:
        pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        index = pc.Index(INDEX_NAME)
    except Exception as e:
        print(f"‚ùå Erreur de connexion Pinecone : {e}")
        return

    # Pr√©paration des embeddings
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vectorstore = PineconeVectorStore(index_name=INDEX_NAME, embedding=embeddings)

    if not os.path.exists(DATA_PATH):
        print(f"‚ùå Dossier '{DATA_PATH}' introuvable.")
        return

    print(f"--- üîÑ Mode Incr√©mental : Mise √† jour dossier {DATA_PATH} ---")
    
    files = [f for f in os.listdir(DATA_PATH) if f.endswith(('.pdf', '.txt', '.csv'))]
    print(f"üìÇ {len(files)} fichiers d√©tect√©s.")

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

    for filename in files:
        file_path = os.path.join(DATA_PATH, filename)
        documents = []
        
        try:
            # A. CHARGEMENT
            if filename.endswith(".pdf"):
                loader = PyPDFLoader(file_path)
                documents = loader.load()
            elif filename.endswith(".txt"):
                loader = TextLoader(file_path, encoding="utf-8")
                documents = loader.load()
            elif filename.endswith(".csv"):
                loader = CSVLoader(file_path, csv_args={"delimiter": ";"}, encoding="latin-1")
                documents = loader.load()

            if not documents:
                print(f"‚ö†Ô∏è  Vide ou illisible : {filename}")
                continue

            # R√©cup√©ration de la source exacte (pour le filtre de suppression)
            # Langchain met le chemin relatif ou absolu dans metadata['source']
            source_id = documents[0].metadata.get("source")
            
            if not source_id:
                print(f"‚ö†Ô∏è  Pas de source d√©tect√©e pour {filename}, on passe.")
                continue

            # B. NETTOYAGE CIBL√â (La magie est ici)
            # On supprime dans Pinecone tout ce qui correspond √† CE fichier
            print(f"üîÑ Traitement de : {filename}")
            
            # On supprime les anciens chunks de ce fichier sp√©cifique
            index.delete(filter={"source": source_id})
            
            # C. D√âCOUPAGE ET ENVOI
            chunks = text_splitter.split_documents(documents)
            
            # Envoi par petits paquets pour √©viter les erreurs de timeout
            batch_size = 100
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                vectorstore.add_documents(batch)
            
            print(f"   ‚úÖ Mis √† jour ({len(chunks)} fragments).")

        except Exception as e:
            print(f"‚ùå Erreur sur {filename}: {e}")

    print("--- Termin√© ! üéâ ---")

if __name__ == "__main__":
    run_incremental_ingestion()