import streamlit as st
import os
import google.generativeai as genai
from pypdf import PdfReader

# --- 1. CONFIGURATION DE LA PAGE ---
st.set_page_config(
    page_title="Expert Social Pro 2026",
    page_icon="‚öñÔ∏è",
    layout="centered"
)

# --- 2. S√âCURIT√â & API ---
# On r√©cup√®re la cl√© depuis les variables d'environnement (d√©finies dans Google Cloud Run ou .env)
api_key = os.environ.get("GOOGLE_API_KEY")

if not api_key:
    st.error("‚ö†Ô∏è Cl√© API introuvable. Assurez-vous d'avoir d√©fini GOOGLE_API_KEY dans les variables d'environnement.")
    st.stop()

# Configuration du mod√®le
genai.configure(api_key=api_key)

# D√©finition du mod√®le sp√©cifique demand√©
MODEL_NAME = "gemini-2.0-flash-exp"

# --- 3. BASE DE CONNAISSANCES (MEMO_CHIFFRES) ---
# Donn√©es de r√©f√©rence int√©gr√©es (PPV 2026, SMIC, Plafonds...)
MEMO_CHIFFRES = """
R√àGLES ET CHIFFRES CL√âS 2026 - SOCIAL FRANCE

1. PRIME DE PARTAGE DE LA VALEUR (PPV) 2024-2026
- Plafond d'exon√©ration : 3 000 ‚Ç¨ (cas g√©n√©ral) ou 6 000 ‚Ç¨ (si accord d'int√©ressement/participation).
- R√©gime Social/Fiscal (Distinction seuil 3 SMIC) :
  * Salaire < 3 SMIC : Exon√©ration totale (Cotisations, CSG/CRDS, Taxe salaires, Imp√¥t revenu).
  * Salaire >= 3 SMIC : Exon√©ration cotisations sociales uniquement. Assujettissement CSG/CRDS et Imp√¥t sur le revenu (sauf si affectation plan √©pargne).
- Forfait Social :
  * < 250 salari√©s : Exon√©r√©.
  * >= 250 salari√©s : 20%.

2. CHIFFRES CL√âS (Estimations/Provisoires pour contexte 2026)
- SMIC Horaire (r√©f√©rence) : ~11,65 ‚Ç¨ (valeur indicative, v√©rifier arr√™t√©).
- Plafond S√©curit√© Sociale (PASS) : R√©f√©rence 2025 ~47 100 ‚Ç¨ (√† ajuster selon publication officielle).

INSTRUCTION : Tu es un Expert Social. Tu dois toujours citer tes sources (Code du travail, BOSS, URSSAF) quand tu r√©ponds.
Si l'utilisateur pose une question sur un document fourni, base-toi PRIORITAIREMENT sur ce document.
"""

# --- 4. GESTION DE LA SESSION (M√âMOIRE) ---
if "messages" not in st.session_state:
    st.session_state.messages = []

if "context_analyzed" not in st.session_state:
    st.session_state.context_analyzed = ""

if "last_uploaded_file" not in st.session_state:
    st.session_state.last_uploaded_file = None

# --- 5. FONCTIONS UTILES ---
def get_gemini_response(user_input, document_content=""):
    """Envoie le contexte et la question √† Gemini"""
    try:
        model = genai.GenerativeModel(MODEL_NAME)
        
        # Construction du prompt syst√®me dynamique
        system_prompt = f"""
        {MEMO_CHIFFRES}
        
        [CONTEXTE SUPPL√âMENTAIRE - DOCUMENT UTILISATEUR]
        {document_content if document_content else "Aucun document fourni."}
        
        [HISTORIQUE DE LA CONVERSATION]
        Prends en compte les √©changes pr√©c√©dents si n√©cessaire.
        """
        
        # On combine l'historique pour le chat (simplifi√© ici pour l'exemple)
        # Id√©alement, on envoie l'historique structur√© √† l'API, 
        # mais ici on concat√®ne pour s'assurer que le document est bien pris en compte √† chaque tour.
        full_prompt = f"{system_prompt}\n\nQuestion utilisateur : {user_input}"
        
        response = model.generate_content(full_prompt)
        return response.text
    except Exception as e:
        return f"Une erreur technique est survenue avec le mod√®le IA : {e}"

# --- 6. INTERFACE UTILISATEUR ---

# Titre principal
st.title("Bienvenue sur votre expert social d√©di√©.")

# BLOC INFORMATION ET ACTIONS (Le fameux cadre bleu/gris)
with st.container(border=True):
    
    # Ligne d'info Ann√©e Fiscale
    col_ico, col_txt = st.columns([0.5, 5])
    with col_ico:
        st.write("üìÖ")
    with col_txt:
        st.write("**Ann√©e Fiscale : 2026**")
        st.caption("Base √† jour.")

    st.write("") # Espaceur

    # ACTION 1 : Nouvelle conversation (Remplace la corbeille)
    if st.button("Nouvelle conversation", use_container_width=True):
        st.session_state.messages = []
        st.session_state.context_analyzed = ""
        st.session_state.last_uploaded_file = None
        st.rerun()

    # ACTION 2 : Upload de document (Analyse pypdf)
    uploaded_file = st.file_uploader(
        "T√©l√©charger un document pour analyse", 
        type=['pdf', 'txt'],
        label_visibility="collapsed"
    )

    # Traitement du fichier upload√©
    if uploaded_file is not None:
        # On ne retraite que si c'est un nouveau fichier
        if st.session_state.last_uploaded_file != uploaded_file.name:
            with st.spinner("Analyse du document en cours..."):
                text_extracted = ""
                try:
                    if uploaded_file.type == "application/pdf":
                        reader = PdfReader(uploaded_file)
                        for page in reader.pages:
                            text_extracted += (page.extract_text() or "") + "\n"
                    else: # TXT
                        text_extracted = uploaded_file.getvalue().decode("utf-8")
                    
                    st.session_state.context_analyzed = text_extracted
                    st.session_state.last_uploaded_file = uploaded_file.name
                    st.toast(f"Document '{uploaded_file.name}' m√©moris√© !", icon="‚úÖ")
                except Exception as e:
                    st.error(f"Erreur lors de la lecture du fichier : {e}")

# Affichage si un document est actif
if st.session_state.context_analyzed:
    st.info(f"üìÇ Document actif : {st.session_state.last_uploaded_file}", icon="‚ÑπÔ∏è")

# --- 7. ZONE DE CHAT ---
# Affichage de l'historique
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Zone de saisie
if prompt := st.chat_input("Posez votre question sociale..."):
    # 1. Afficher le message utilisateur
    with st.chat_message("user"):
        st.markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 2. G√©n√©rer la r√©ponse
    with st.chat_message("assistant"):
        with st.spinner("Recherche dans les textes officiels..."):
            response_text = get_gemini_response(prompt, st.session_state.context_analyzed)
            st.markdown(response_text)
    
    st.session_state.messages.append({"role": "assistant", "content": response_text})