import os
import sys
import shutil

# Importations n√©cessaires
try:
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_google_genai import GoogleGenerativeAIEmbeddings
    from langchain_chroma import Chroma
except ImportError as e:
    print(f"‚ùå Erreur d'importation : {e}")
    sys.exit(1)

# --- CONFIGURATION CL√â API ---
# On r√©cup√®re la cl√© inject√©e par Google Cloud Build via l'environnement
MY_API_KEY = os.environ.get("GOOGLE_API_KEY")
if not MY_API_KEY:
    print("‚ùå ERREUR CRITIQUE : Variable GOOGLE_API_KEY introuvable.")
    sys.exit(1)

os.environ["GOOGLE_API_KEY"] = MY_API_KEY

SOURCE_DIRECTORY = "sources_pdf"
PERSIST_DIRECTORY = "chroma_db"

def main():
    print("üöÄ D√©marrage de la vectorisation optimis√©e (Mode Batch)...")

    # 1. V√©rifications de base
    if not os.path.exists(SOURCE_DIRECTORY):
        print(f"‚ùå Le dossier '{SOURCE_DIRECTORY}' n'existe pas.")
        sys.exit(1)

    if os.path.exists(PERSIST_DIRECTORY):
        print(f"üßπ Nettoyage de l'ancienne base...")
        shutil.rmtree(PERSIST_DIRECTORY)

    # 2. Chargement d√©taill√© des PDF
    files = [f for f in os.listdir(SOURCE_DIRECTORY) if f.lower().endswith('.pdf')]
    print(f"üìÇ {len(files)} fichiers trouv√©s dans '{SOURCE_DIRECTORY}'")

    all_documents = []
    for i, filename in enumerate(files, 1):
        file_path = os.path.join(SOURCE_DIRECTORY, filename)
        print(f"[{i}/{len(files)}] üìñ Lecture de : {filename}...", end=" ", flush=True)
        try:
            loader = PyPDFLoader(file_path)
            docs = loader.load()
            all_documents.extend(docs)
            print(f"‚úÖ ({len(docs)} pages)")
        except Exception as e:
            print(f"‚ùå Erreur sur ce fichier : {e}")

    if not all_documents:
        print("‚ö†Ô∏è Aucun contenu extrait. Fin du script.")
        sys.exit(1)

    # 3. D√©coupage en fragments
    print(f"‚úÇÔ∏è D√©coupage de {len(all_documents)} pages en fragments...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    chunks = text_splitter.split_documents(all_documents)
    print(f"üß© {len(chunks)} fragments cr√©√©s.")

    # 4. Vectorisation par paquets (Batching)
    print("üß† G√©n√©ration des embeddings et cr√©ation de la base...")
    # Utilisation du mod√®le recommand√© text-embedding-004
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

    batch_size = 50  # On envoie 50 fragments par 50 pour la stabilit√©
    vectorstore = None
    
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i : i + batch_size]
        current_batch_num = i // batch_size + 1
        total_batches = (len(chunks) // batch_size) + 1
        
        print(f"   üì§ Envoi du paquet {current_batch_num}/{total_batches}... ", end="", flush=True)
        
        try:
            if vectorstore is None:
                vectorstore = Chroma.from_documents(
                    documents=batch,
                    embedding=embeddings,
                    persist_directory=PERSIST_DIRECTORY
                )
            else:
                vectorstore.add_documents(documents=batch)
            print("‚úÖ")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'envoi du paquet : {e}")
            sys.exit(1)

    print("-" * 50)
    print(f"‚úÖ SUCC√àS TOTAL : Base cr√©√©e avec {len(chunks)} fragments.")
    print("-" * 50)

if __name__ == "__main__":
    main()